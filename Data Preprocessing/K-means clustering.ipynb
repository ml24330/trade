{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "articles = pd.read_csv('articles.csv', index_col='Identifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/liumukun/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/liumukun/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to process text\n",
    "\n",
    "import string\n",
    "custom_stopwords = ['party', 'agreement', 'chapter', 'article']\n",
    "def text_process(text):\n",
    "    stemmer = WordNetLemmatizer()\n",
    "    nopunc = [char for char in text if char not in string.punctuation]\n",
    "    nopunc = ''.join([i for i in nopunc if not i.isdigit()])\n",
    "    nopunc =  [word.lower() for word in nopunc.split() if word not in stopwords.words('english')]\n",
    "    nopunc =  [word.lower() for word in nopunc if word not in custom_stopwords]\n",
    "    nopunc =  [word.lower() for word in nopunc if len(word) > 3]\n",
    "    return [stemmer.lemmatize(word) for word in nopunc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process articles\n",
    "\n",
    "processed = articles.apply(lambda article: text_process(article), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'processed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b4011c16bb91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtfidfconvert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'word'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mngram_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marticle\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprocessed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mX_transformed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidfconvert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marticle\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprocessed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'processed' is not defined"
     ]
    }
   ],
   "source": [
    "# Convert to tf-idf\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidfconvert = TfidfVectorizer(analyzer='word',ngram_range=(1,1)).fit([' '.join(article) for article in processed])\n",
    "X_transformed = tfidfconvert.transform([' '.join(article) for article in processed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create function to map indices to words\n",
    "\n",
    "def create_map(vocab, clusters):\n",
    "    inv_map = {v: k for k, v in vocab.items()} \n",
    "    dicts = [{index: value for (index, value) in enumerate(cluster) if value > 0} for cluster in clusters]\n",
    "    sorted_dicts = [{k: v for k, v in sorted(_dict.items(), key=lambda item: item[-1], reverse=True)} for _dict in dicts]\n",
    "    labeled_dicts = [{inv_map[k]: v for k, v in _dict.items()} for _dict in sorted_dicts]\n",
    "    first_10_pairs = [{k: _dict[k] for k in list(_dict)[:10]} for _dict in labeled_dicts]\n",
    "    return first_10_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster using K-means\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, init='k-means++', n_init=100).fit(X_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'shall': 0.06292169474192705, 'party': 0.053846239535849855, 'agreement': 0.036737808909043095, 'trade': 0.03372382226676572, 'measure': 0.030769635384145243, 'committee': 0.029785631454605548, 'information': 0.021826014150818186, 'parties': 0.021374302282514312, 'joint': 0.021108682599401886, 'request': 0.020784063847591523}\n",
      "{'service': 0.16199964372517123, 'supplier': 0.09969548725109252, 'financial': 0.08001410124718303, 'investment': 0.06461323724141846, 'investor': 0.06446557741565448, 'telecommunication': 0.05512211819372553, 'shall': 0.04650619958804002, 'party': 0.04598090403023955, 'territory': 0.044903340247931925, 'enterprise': 0.0444125956588261}\n",
      "{'good': 0.13590443955421883, 'custom': 0.0827907131588339, 'duty': 0.07133322186241634, 'originating': 0.06842474381022443, 'shall': 0.058925983096964035, 'product': 0.05575777490867084, 'material': 0.04741648037205108, 'party': 0.04489974875943644, 'territory': 0.04442196779035106, 'origin': 0.03873251498589244}\n"
     ]
    }
   ],
   "source": [
    "# Print top words in each cluster\n",
    "\n",
    "print(create_map(tfidfconvert.vocabulary_, kmeans.cluster_centers_)[0])\n",
    "print(create_map(tfidfconvert.vocabulary_, kmeans.cluster_centers_)[1])\n",
    "print(create_map(tfidfconvert.vocabulary_, kmeans.cluster_centers_)[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Label articles\n",
    "\n",
    "df = pd.DataFrame(processed, columns=['Article'])\n",
    "df = df.reset_index()\n",
    "df['Class 0'] = pd.Series(kmeans.labels_) == 0\n",
    "df['Class 1'] = pd.Series(kmeans.labels_) == 1\n",
    "df['Class 2'] = pd.Series(kmeans.labels_) == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label agreements\n",
    "\n",
    "classified = df.groupby('Identifier').sum()\n",
    "total = classified['Class 0']+classified['Class 1']+classified['Class 2']\n",
    "classified['Class 0'] = classified['Class 0']/total\n",
    "classified['Class 1'] = classified['Class 1']/total\n",
    "classified['Class 2'] = classified['Class 2']/total\n",
    "classified.to_csv('unsupervised.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
